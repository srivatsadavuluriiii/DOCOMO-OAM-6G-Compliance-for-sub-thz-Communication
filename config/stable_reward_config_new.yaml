# OAM 6G Stable Reward Configuration
# Inherits from rl_config_new.yaml
# Contains ONLY stable reward-specific parameters

# Stable reward function parameters (stable reward-specific)
stable_reward:
  smoothing_factor: 0.7        # Exponential smoothing factor (0-1)
  window_size: 10              # Size of moving average window
  reward_scale: 2.0            # Scaling factor for reward
  reward_min: -10.0            # Minimum reward value
  reward_max: 10.0             # Maximum reward value
  sinr_scaling_factor: 0.1     # Scale SINR contribution to reward

# Override reward parameters for stable reward (different from standard RL)
reward:
  throughput_factor: 5.0       # Increased weight for throughput
  handover_penalty: 0.5        # Penalty for mode handovers
  outage_penalty: 5.0          # Penalty for SINR below threshold
  sinr_threshold: -5.0         # SINR threshold for outage (dB)

# Note: The following parameters are inherited from rl_config_new.yaml:
# - training: num_episodes, max_steps_per_episode, batch_size, learning_rate, gamma, target_update_freq
# - system: frequency, bandwidth, tx_power_dBm, noise_figure_dB, noise_temp
# - oam: min_mode, max_mode, mode_spacing, beam_width
# - environment: pointing_error_std, rician_k_factor, turbulence_strength, humidity, temperature, pressure
# - enhanced_params: antenna_efficiency, implementation_loss_dB
# - mobility: max_speed, min_speed, direction_change_prob
# - rl_base.network: action_dim, hidden_layers, activation
# - rl_base.replay_buffer: capacity, min_samples_to_learn
# - rl_base.exploration: epsilon_start, epsilon_end, epsilon_decay
# - rl_base.evaluation: eval_episodes, eval_frequency, save_model_frequency
# - rl_base.reward: throughput_factor, handover_penalty, outage_penalty, sinr_threshold (overridden above) 